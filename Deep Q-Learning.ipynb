{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Deep Q-Learning.ipynb","private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyM6EbzCq/Rko07gduWWPYHv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"6hjWn0gLHO3k"},"source":["Deep Q- Learning\n","Deep Reinforcement Learning (심층강화학습)\n","\n","-강화학습이란\n","강화학습 (Reinforcement Learning) 은 기계학습(Machine Learning)의 한 종류.\n","환경(입출력)에서 상호 작용하여 학습하는 Agent 제작.\n","Agent는 시행착오를 통해 학습을 하고, 많은 경험을 하게 됨.\n","(아기가 처음 걷는 법과 같은 학습 방법. 넘어지면서 여러 번 시도 끝에 걷는 것을 배움.)\n","\n","-Deep Q Network (DQN)\n","13년도에 구글이 발표한 새로운 알고리즘.\n","Agent가 화면을 관찰하는 것만으로 어떻게 게임을 배울 수 있는지 보여줌.\n","딥러닝 + 강화학습 = Deep Reinforcement Learning\n","DQN 알고리즘의 신경망은 환경을 기반으로 최고의 동작을 수행하는데 사용됨. (State)\n","Q함수는 State를 기반으로 잠재적인 보상을 추정하는 데 사용됨. = Q (State, Action)\n","Q는 State와 Action을 기준으로 예상되는 미래의 값들을 계산하는 함수.\n","\n","\n","\n","-CartPole\n","목표 : 움직이는 카트 위에 있는 폴의 균형을 맞추는 것.\n","\n","Gym은 게임 환경과의 모든 상호작용을 단순화함. Agent의 두뇌에 초점을 맞춤.\n"," \n"]},{"cell_type":"code","metadata":{"id":"lqAfqPoIIpys"},"source":["import keras\n","import gym"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VqTpJfVCHJlU"},"source":["# INPUT\n","# action은 0 또는 1\n","\n","# OUTPUT\n","# 다음 State, 보상, 정보 : 우리가 무엇을 위해서 학습하는지에 대한 것\n","# done : 게임이 끝났는지 아닌지에 대한 boolean 타입의 값\n","next_state, reward, done, info = gym.Env.step(action)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BlCfzQIGHPAs"},"source":["#신경망만들기\n","-신경망이란\n","데이터 쌍(입력 및 출력 데이터)를 기반으로 학습. 특정 유형의 패턴 감지, 또 다른 입력 데이터를 기반으로 출력을 예측하는 알고리즘.\n","\n","4개의 정보를 받는 입력 레이어, 3개의 히든레이어, 게임 버튼이 2개(0과 1)이므로 출력 레이어 2개.\n"]},{"cell_type":"code","metadata":{"id":"Ch9X1zp8HjTv"},"source":["# Deep Q Learning 을 위한 신경망 만들기\n","# Sequential() 은 레이어를 쌓아줍니다.\n","model = Sequential()\n","# ‘Dense’ 는 신경망의 기본 form 입니다.\n","# State 의 입력 사이즈는 size(4) 이고, 히든 레이어는 24개의 노드로 이루어집니다.\n","model.add(Dense(24, input_dim=self.state_size, activation=’relu’))\n","# 24개의 노드를 가진 히든 레이어\n","model.add(Dense(24, activation=’relu’))\n","# 출력 레이어 : 2개의 노드 (left, right)\n","model.add(Dense(self.action_size, activation=’linear’))\n","# 정보를 토대로 모델을 만듭니다.\n","model.compile(loss=’mse’, optimizer=Adam(lr=self.learning_rate))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BzvEhcEgHrIm"},"source":["#신경망 학습\n","\n","모델에 데이터를 입력해야함.\n","케라스는 입력 및 출력 쌍을 모델에 공급하는 `fit()`을 제공함.\n","그런 다음 모델은 이러한 데이터를 기반으로 학습되고, 입력을 기준으로 출력을 추정함.\n","\n","이 학습 과정에서 신경망이 `state`로부터 보상을 예측할 수 있게함.\n"]},{"cell_type":"code","metadata":{"id":"Z54puXiyIK4_"},"source":["model.fit(state, reward_value, epochs=1, verbose=0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C4qnZPwYIH-s"},"source":["#예측\n","모델 학습 후, 입력 데이터로부터 출력(결과)을 예측할 수 있음.\n","`predict()` 함수는 모델이 훈련된 데이터를 기반으로 현재 `state`의 보상을 예측함."]},{"cell_type":"code","metadata":{"id":"NVuzXlBxIY8f"},"source":["prediction = model.predict(state)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OMw6eceZB5If"},"source":["#전체코드\n"]},{"cell_type":"code","metadata":{"id":"AAzIAHoSB64Y"},"source":["import gym\n","import random\n","import os\n","import numpy as np\n","from collections      import deque\n","from keras.models     import Sequential\n","from keras.layers     import Dense\n","from keras.optimizers import Adam\n","\n","class Agent():\n","    def __init__(self, state_size, action_size):\n","        self.weight_backup      = \"cartpole_weight.h5\"\n","        self.state_size         = state_size\n","        self.action_size        = action_size\n","        self.memory             = deque(maxlen=2000)\n","        self.learning_rate      = 0.001\n","        self.gamma              = 0.95\n","        self.exploration_rate   = 1.0\n","        self.exploration_min    = 0.01\n","        self.exploration_decay  = 0.995\n","        self.brain              = self._build_model()\n","\n","    def _build_model(self):\n","        # Neural Net for Deep-Q learning Model\n","        model = Sequential()\n","        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n","        model.add(Dense(24, activation='relu'))\n","        model.add(Dense(self.action_size, activation='linear'))\n","        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n","\n","        if os.path.isfile(self.weight_backup):\n","            model.load_weights(self.weight_backup)\n","            self.exploration_rate = self.exploration_min\n","        return model\n","\n","    def save_model(self):\n","            self.brain.save(self.weight_backup)\n","\n","    def act(self, state):\n","        if np.random.rand() <= self.exploration_rate:\n","            return random.randrange(self.action_size)\n","        act_values = self.brain.predict(state)\n","        return np.argmax(act_values[0])\n","\n","    def remember(self, state, action, reward, next_state, done):\n","        self.memory.append((state, action, reward, next_state, done))\n","\n","    def replay(self, sample_batch_size):\n","        if len(self.memory) < sample_batch_size:\n","            return\n","        sample_batch = random.sample(self.memory, sample_batch_size)\n","        for state, action, reward, next_state, done in sample_batch:\n","            target = reward\n","            if not done:\n","              target = reward + self.gamma * np.amax(self.brain.predict(next_state)[0])\n","            target_f = self.brain.predict(state)\n","            target_f[0][action] = target\n","            self.brain.fit(state, target_f, epochs=1, verbose=0)\n","        if self.exploration_rate > self.exploration_min:\n","            self.exploration_rate *= self.exploration_decay\n","\n","class CartPole:\n","    def __init__(self):\n","        self.sample_batch_size = 32\n","        self.episodes          = 10000\n","        self.env               = gym.make('CartPole-v1')\n","\n","        self.state_size        = self.env.observation_space.shape[0]\n","        self.action_size       = self.env.action_space.n\n","        self.agent             = Agent(self.state_size, self.action_size)\n","\n","\n","    def run(self):\n","        try:\n","            for index_episode in range(self.episodes):\n","                state = self.env.reset()\n","                state = np.reshape(state, [1, self.state_size])\n","\n","                done = False\n","                index = 0\n","                while not done:\n","                    self.env.render()\n","\n","                    action = self.agent.act(state)\n","\n","                    next_state, reward, done, _ = self.env.step(action)\n","                    next_state = np.reshape(next_state, [1, self.state_size])\n","                    self.agent.remember(state, action, reward, next_state, done)\n","                    state = next_state\n","                    index += 1\n","                print(\"Episode {}# Score: {}\".format(index_episode, index + 1))\n","                self.agent.replay(self.sample_batch_size)\n","        finally:\n","            self.agent.save_model()\n","\n","if __name__ == \"__main__\":\n","    cartpole = CartPole()\n","    cartpole.run()"],"execution_count":null,"outputs":[]}]}
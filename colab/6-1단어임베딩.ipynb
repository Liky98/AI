{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"6-1단어임베딩.ipynb","provenance":[],"authorship_tag":"ABX9TyMxhbi1bf8+yxszt1lPauGF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"iF9dNevHVvhP"},"source":["## 단어수준 원핫인코딩"]},{"cell_type":"code","metadata":{"id":"ewsov87AQ0Se"},"source":["\n","import numpy as np\n","\n","samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n","\n","token_index = {} #딕셔너리 ex) {key1:1, key2:2}\n","for sample in samples:\n","    for word in sample.split():\n","        if word not in token_index:\n","            token_index[word] = len(token_index) + 1\n","            # token_index =  {The : 1, cat : 2}\n","            # 0번째는 사용안함\n","max_length = 10\n","print(token_index)\n","\n","results = np.zeros((len(samples), max_length, max(token_index.values()) + 1)) #[2][10][11]\n","\n","for i, sample in enumerate(samples): # 0 : the cat sat on the  mat , 1 : the dog ... 출력. 순서와 리스트의 값을 전달하는 함수\n","    print(i, end = \" \")\n","    print(sample)\n","    for j, word in list(enumerate(sample.split()))[:max_length]: #최대단어길이\n","        index = token_index.get(word)\n","        results[i, j, index] = 1.\n","        print(j, end = \" \")\n","        print(word)\n","\n","print(results)#벡터화"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jucoV_XOVwy_"},"source":["##문자수준 원핫인코딩"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BzBaYt0oVmmQ","executionInfo":{"status":"ok","timestamp":1622798994737,"user_tz":-540,"elapsed":272,"user":{"displayName":"이기훈/컴퓨터공학과","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgCCfJGhK9OkO-XoYspoYuzjHaz1LPVg5qTnbT0=s64","userId":"10762617231091760260"}},"outputId":"b1b23bcc-3947-434d-92cc-b55226b7210c"},"source":["import string\n","\n","samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n","characters = string.printable  # 출력 가능한 모든 아스키(ASCII) 문자\n","token_index = dict(zip(characters, range(1, len(characters) + 1)))#zip(iterable)\n","\n","max_length = 50\n","results = np.zeros((len(samples), max_length, max(token_index.values()) + 1))\n","for i, sample in enumerate(samples):\n","    for j, character in enumerate(sample[:max_length]):\n","        index = token_index.get(character)\n","        results[i, j, index] = 1.\n","\n","print(token_index)\n","print(results.shape)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'0': 1, '1': 2, '2': 3, '3': 4, '4': 5, '5': 6, '6': 7, '7': 8, '8': 9, '9': 10, 'a': 11, 'b': 12, 'c': 13, 'd': 14, 'e': 15, 'f': 16, 'g': 17, 'h': 18, 'i': 19, 'j': 20, 'k': 21, 'l': 22, 'm': 23, 'n': 24, 'o': 25, 'p': 26, 'q': 27, 'r': 28, 's': 29, 't': 30, 'u': 31, 'v': 32, 'w': 33, 'x': 34, 'y': 35, 'z': 36, 'A': 37, 'B': 38, 'C': 39, 'D': 40, 'E': 41, 'F': 42, 'G': 43, 'H': 44, 'I': 45, 'J': 46, 'K': 47, 'L': 48, 'M': 49, 'N': 50, 'O': 51, 'P': 52, 'Q': 53, 'R': 54, 'S': 55, 'T': 56, 'U': 57, 'V': 58, 'W': 59, 'X': 60, 'Y': 61, 'Z': 62, '!': 63, '\"': 64, '#': 65, '$': 66, '%': 67, '&': 68, \"'\": 69, '(': 70, ')': 71, '*': 72, '+': 73, ',': 74, '-': 75, '.': 76, '/': 77, ':': 78, ';': 79, '<': 80, '=': 81, '>': 82, '?': 83, '@': 84, '[': 85, '\\\\': 86, ']': 87, '^': 88, '_': 89, '`': 90, '{': 91, '|': 92, '}': 93, '~': 94, ' ': 95, '\\t': 96, '\\n': 97, '\\r': 98, '\\x0b': 99, '\\x0c': 100}\n","(2, 50, 101)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rUdSsVEfZVsn"},"source":["## 케라스 사용 단어 수준 원핫인코딩"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SfrsleXAZXte","executionInfo":{"status":"ok","timestamp":1622799771374,"user_tz":-540,"elapsed":262,"user":{"displayName":"이기훈/컴퓨터공학과","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgCCfJGhK9OkO-XoYspoYuzjHaz1LPVg5qTnbT0=s64","userId":"10762617231091760260"}},"outputId":"25dbbe94-1ed8-4bdd-9eb1-da0d0c7064f4"},"source":["from keras.preprocessing.text import Tokenizer\n","\n","samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n","\n","# 가장 빈도가 높은 1,000개의 단어만 선택하도록 Tokenizer 객체를 만듭니다.\n","tokenizer = Tokenizer(num_words=1000)\n","\n","# 단어 인덱스를 구축합니다.\n","tokenizer.fit_on_texts(samples)\n","\n","# 문자열을 정수 인덱스의 리스트로 변환합니다.\n","sequences = tokenizer.texts_to_sequences(samples)\n","print(sequences)\n","\n","# 직접 원-핫 이진 벡터 표현을 얻을 수 있습니다.\n","# 원-핫 인코딩 외에 다른 벡터화 방법들도 제공합니다!\n","one_hot_results = tokenizer.texts_to_matrix(samples, mode='binary')\n","\n","# 계산된 단어 인덱스를 구합니다.\n","word_index = tokenizer.word_index\n","print('Found %s unique tokens.' % len(word_index))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[1, 2, 3, 4, 1, 5], [1, 6, 7, 8, 9]]\n","Found 9 unique tokens.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3ZGxiwK5bhbO"},"source":["## 해싱 기법 단어 수준 원핫인코딩\n","원-핫 인코딩의 변종 중 하나는 원-핫 해싱 기법입니다.\n","\n","어휘 사전에 있는 고유한 토큰의 수가 너무 커서 모두 다루기 어려울 때 사용. \n","\n","각 단어에 명시적으로 인덱스를 할당하고 이 인덱스를 딕셔너리에 저장하는 대신에 단어를 해싱하여 고정된 크기의 벡터로 변환합니다.\n","\n"," 일반적으로 간단한 해싱 함수를 사용합니다.\n","\n","  이 방식의 주요 장점은 명시적인 단어 인덱스가 필요 없기 때문에 메모리를 절약하고 온라인 방식으로 데이터를 인코딩할 수 있습니다(전체 데이터를 확인하지 않고 토큰을 생성할 수 있습니다). \n","\n","  한 가지 단점은 해시 충돌입니다. 두 개의 단어가 같은 해시를 만들면 이를 바라보는 머신 러닝 모델은 단어 사이의 차이를 인식하지 못합니다. \n","\n","  해싱 공간의 차원이 해싱될 고유 토큰의 전체 개수보다 훨씬 크면 해시 충돌의 가능성은 감소합니다."]},{"cell_type":"code","metadata":{"id":"j02ILxcXbz9v"},"source":["samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n","\n","# 단어를 크기가 1,000인 벡터로 저장합니다.\n","# 1,000개(또는 그이상)의 단어가 있다면 해싱 충돌이 늘어나고 인코딩의 정확도가 감소될 것입니다\n","dimensionality = 1000\n","max_length = 10\n","\n","results = np.zeros((len(samples), max_length, dimensionality))\n","for i, sample in enumerate(samples):\n","    for j, word in list(enumerate(sample.split()))[:max_length]:\n","        # 단어를 해싱하여 0과 1,000 사이의 랜덤한 정수 인덱스로 변환합니다.\n","        index = abs(hash(word)) % dimensionality\n","        results[i, j, index] = 1."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tSW6VrPicWfm"},"source":["## 단어임베딩 사용"]},{"cell_type":"markdown","metadata":{"id":"UgW406n3cb4X"},"source":["단어와 벡터를 연관짓는 강력하고 인기 있는 또 다른 방법은 단어 임베딩이라는 밀집 단어 벡터를 사용하는 것입니다. \n","\n","원-핫 인코딩으로 만든 벡터는 희소하고(대부분 0으로 채워집니다) 고차원입니다(어휘 사전에 있는 단어의 수와 차원이 같습니다). \n","\n","반면 **단어 임베딩은 저차원의 실수형 벡터**입니다(희소 벡터의 반대인 밀집 벡터입니다). \n","\n","원-핫 인코딩으로 얻은 단어 벡터와 달리 단어 임베딩은 데이터로부터 학습됩니다.\n","\n"," 보통 256차원, 512차원 또는 큰 어휘 사전을 다룰 때는 1,024차원의 단어 임베딩을 사용합니다.\n"," \n","  반면 원-핫 인코딩은 (20,000개의 토큰으로 이루어진 어휘 사전을 만들려면) 20,000차원 또는 그 이상의 벡터일 경우가 많습니다. \n","  \n","따라서 단어 임베딩이 더 많은 정보를 적은 차원에 저장합니다."]},{"cell_type":"markdown","metadata":{"id":"tnwoTNR2c5Dm"},"source":["단어 임베딩을 만드는 방법은 두 가지입니다.\n","\n","(문서 분류나 감성 예측과 같은) 관심 대상인 문제와 함께 단어 임베딩을 학습합니다. \n","\n","이런 경우에는 랜덤한 단어 벡터로 시작해서 신경망의 가중치를 학습하는 것과 같은 방식으로 단어 벡터를 학습합니다.\n","\n","\n","풀려는 문제가 아니고 다른 머신 러닝 작업에서 미리 계산된 단어 임베딩을 로드합니다.\n","\n"," 이를 사전 훈련된 단어 임베딩이라고 합니다."]},{"cell_type":"code","metadata":{"id":"PhgHWCNOcYsX"},"source":["\n","from keras.layers import Embedding\n","\n","# Embedding 층은 적어도 두 개의 매개변수를 받습니다.\n","# 가능한 토큰의 개수(여기서는 1,000으로 단어 인덱스 최댓값 + 1입니다)와 임베딩 차원(여기서는 64)입니다\n","embedding_layer = Embedding(1000, 64)# (samples, sequence_length)인 2D 정수 텐서\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sIolssmht9tM"},"source":["배치에 있는 모든 시퀀스는 길이가 같아야 하므로(하나의 텐서에 담아야 하기 때문에) 작은 길이의 시퀀스는 0으로 패딩되고 길이가 더 긴 시퀀스는 잘립니다.\n","\n"," (samples, sequence_length, embedding_dimensionality)인 3D 실수형 텐서를 반환\n","\n"," 이 3D 텐서는 RNN 층이나 1D 합성곱 층에서 처리\n","\n"," Embedding 층의 객체를 생성할 때 가중치(토큰 벡터를 위한 내부 딕셔너리)는 다른 층과 마찬가지로 랜덤하게 초기화\n","\n"," 단어 벡터는 역전파를 통해 점차 조정되어 이어지는 모델이 사용할 수 있도록 임베팅 공간을 구성\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0kvHAfVNt-kM","executionInfo":{"status":"ok","timestamp":1622806890730,"user_tz":-540,"elapsed":6376,"user":{"displayName":"이기훈/컴퓨터공학과","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgCCfJGhK9OkO-XoYspoYuzjHaz1LPVg5qTnbT0=s64","userId":"10762617231091760260"}},"outputId":"01205353-c0e5-4362-e9d3-e01e44af469e"},"source":["from keras.datasets import imdb\n","from keras import preprocessing\n","\n","# 특성으로 사용할 단어의 수\n","max_features = 10000\n","# 사용할 텍스트의 길이(가장 빈번한 max_features 개의 단어만 사용합니다)\n","maxlen = 20\n","\n","# 정수 리스트로 데이터를 로드합니다.\n","(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n","\n","# 리스트를 (samples, maxlen) 크기의 2D 정수 텐서로 변환합니다.\n","x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n","x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)\n","print(x_train.shape)\n","print(x_test.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","/usr/local/lib/python3.7/dist-packages/keras/datasets/imdb.py:155: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n","/usr/local/lib/python3.7/dist-packages/keras/datasets/imdb.py:156: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"],"name":"stderr"},{"output_type":"stream","text":["(25000, 20)\n","(25000, 20)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9P6b2VSowBB7"},"source":["영화 리뷰에서 가장 빈도가 높은 10,000개의 단어를 추출하고(처음 이 데이터셋으로 작업했던 것과 동일합니다) 리뷰에서 20개 단어 이후는 버립니다. 이 네트워크는 10,000개의 단어에 대해 8 차원의 임베딩을 학습하여 정수 시퀀스 입력(2D 정수 텐서)를 임베딩 시퀀스(3D 실수형 텐서)로 바꿀 것입니다. 그 다음 이 텐서를 2D로 펼쳐서 분류를 위한 Dense 층을 훈련하겠습니다."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SfoUVt9YwPQ8","executionInfo":{"status":"ok","timestamp":1622806770308,"user_tz":-540,"elapsed":34169,"user":{"displayName":"이기훈/컴퓨터공학과","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgCCfJGhK9OkO-XoYspoYuzjHaz1LPVg5qTnbT0=s64","userId":"10762617231091760260"}},"outputId":"083eb5ee-357b-46e6-8b01-ee7d3a3515a4"},"source":["from keras.models import Sequential\n","from keras.layers import Flatten, Dense, Embedding\n","\n","model = Sequential()\n","# 나중에 임베딩된 입력을 Flatten 층에서 펼치기 위해 Embedding 층에 input_length를 지정합니다.\n","model.add(Embedding(10000, 8, input_length=maxlen))\n","# Embedding 층의 출력 크기는 (samples, maxlen, 8)가 됩니다.\n","# 8 차원의 임베딩을 학습\n","\n","# 3D 임베딩 텐서를 (samples, maxlen * 8) 크기의 2D 텐서로 펼칩니다.\n","model.add(Flatten())\n","\n","# 분류기를 추가합니다.\n","model.add(Dense(1, activation='sigmoid'))\n","model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n","model.summary()\n","\n","history = model.fit(x_train, y_train,\n","                    epochs=10,\n","                    batch_size=32,\n","                    validation_split=0.2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_3 (Embedding)      (None, 20, 8)             80000     \n","_________________________________________________________________\n","flatten (Flatten)            (None, 160)               0         \n","_________________________________________________________________\n","dense (Dense)                (None, 1)                 161       \n","=================================================================\n","Total params: 80,161\n","Trainable params: 80,161\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/10\n","625/625 [==============================] - 15s 2ms/step - loss: 0.6812 - acc: 0.5845 - val_loss: 0.5941 - val_acc: 0.7086\n","Epoch 2/10\n","625/625 [==============================] - 1s 2ms/step - loss: 0.5435 - acc: 0.7552 - val_loss: 0.5147 - val_acc: 0.7332\n","Epoch 3/10\n","625/625 [==============================] - 1s 2ms/step - loss: 0.4596 - acc: 0.7871 - val_loss: 0.4955 - val_acc: 0.7472\n","Epoch 4/10\n","625/625 [==============================] - 1s 2ms/step - loss: 0.4208 - acc: 0.8118 - val_loss: 0.4931 - val_acc: 0.7500\n","Epoch 5/10\n","625/625 [==============================] - 1s 2ms/step - loss: 0.3965 - acc: 0.8238 - val_loss: 0.4931 - val_acc: 0.7514\n","Epoch 6/10\n","625/625 [==============================] - 1s 2ms/step - loss: 0.3736 - acc: 0.8351 - val_loss: 0.4955 - val_acc: 0.7516\n","Epoch 7/10\n","625/625 [==============================] - 1s 2ms/step - loss: 0.3619 - acc: 0.8406 - val_loss: 0.4996 - val_acc: 0.7552\n","Epoch 8/10\n","625/625 [==============================] - 1s 2ms/step - loss: 0.3421 - acc: 0.8532 - val_loss: 0.5048 - val_acc: 0.7536\n","Epoch 9/10\n","625/625 [==============================] - 1s 2ms/step - loss: 0.3272 - acc: 0.8629 - val_loss: 0.5087 - val_acc: 0.7540\n","Epoch 10/10\n","625/625 [==============================] - 1s 2ms/step - loss: 0.3137 - acc: 0.8687 - val_loss: 0.5153 - val_acc: 0.7536\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nhpq2Yfk3XJT"},"source":["##.원본텍스트에서 단어 임베딩까지\n"]},{"cell_type":"code","metadata":{"id":"4UqNmTx_3bEL"},"source":["\n","labels = []\n","texts = []\n","\n","for label_type in ['neg', 'pos']:\n","    dir_name = os.path.join(train_dir, label_type)\n","    for fname in os.listdir(dir_name):\n","        if fname[-4:] == '.txt':\n","            f = open(os.path.join(dir_name, fname), encoding='utf8')\n","            texts.append(f.read())\n","            f.close()\n","            if label_type == 'neg':\n","                labels.append(0)\n","            else:\n","                labels.append(1)"],"execution_count":null,"outputs":[]}]}